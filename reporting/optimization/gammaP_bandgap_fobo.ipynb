{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce096015-21cf-4974-9a07-c9c0d13a057b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BO with Derivatives for Gamma point bandgap optimization (silicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3ae78-4f7c-49d4-a5ed-48b8b717734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.acquisition import qKnowledgeGradient, qNoisyExpectedImprovement\n",
    "from botorch.acquisition.objective import ScalarizedPosteriorTransform\n",
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.models.model import FantasizeMixin\n",
    "from botorch.models.gpytorch import GPyTorchModel\n",
    "\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import ExactMarginalLogLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c38a9d-2c6e-4804-a43e-ab1a9ca1e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default data type to avoid numerical issues from low precision\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "# Set seed for consistency and good results\n",
    "seed = 3\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0682efe-5d3d-4128-88fe-6783aec381bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "f_noise = 0.05 # Noise added to both function evaluations and their gradients\n",
    "rs_init = 10 # Initialization points from random search\n",
    "rb_dim = 3 # Rosenbrock dimensions\n",
    "d_min = -2. # Lower bound of search space for each dimension of the domain\n",
    "d_max = 2. # Upper bound of search space for each dimension of the domain\n",
    "bo_iters = 40 # Number of iterations of Bayesian Optimization\n",
    "mc_samples = 256# Samples from Monte-Carlo sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f873c969-0fd5-4b5b-9b4b-3259c7157c61",
   "metadata": {},
   "source": [
    "## Function to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd5212d-b0d5-4c04-89a1-2ad5704a0ca1",
   "metadata": {},
   "source": [
    "### Load Julia backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe6992c-2689-4e5e-add1-6f7f3fe5f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from julia import Julia, Pkg, Main\n",
    "jl = Julia(runtime=julia_runtime_path, compiled_modules=False)\n",
    "\n",
    "# Load InverseDesign.jl\n",
    "pkg_path = os.path.join(julia_molsim_folder, \"InverseDesign.jl/\")\n",
    "Pkg.activate(pkg_path)\n",
    "Pkg.resolve()\n",
    "Pkg.instantiate()\n",
    "\n",
    "Main.include(os.path.join(pkg_path, \"examples/python_helpers.jl\"))\n",
    "%load_ext julia.magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c06a83-8480-49a5-8137-2b5d35a9ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 0.05\n",
    "def evaluate(parameters):\n",
    "    # Only vary stress along x and y.\n",
    "    x = np.array(np.squeeze([[parameters.get(f\"x{i+1}\") for i in range(2)] + [0, 0, 0, 0]]))\n",
    "    # The GaussianLikelihood used by our model infers an observation noise level,\n",
    "    # so we pass an sem value of NaN to indicate that observation noise is unknown\n",
    "    bandgap_w_grad = %julia gamma_point_bandgap_vs_strain12($x)\n",
    "    bandgap, bandgap_grad = bandgap_w_grad[0], bandgap_w_grad[1:]\n",
    "    loss = np.abs(bandgap - target) # optimize to target value.\n",
    "    Y = loss\n",
    "    grad = bandgap_grad if bandgap > target else - bandgap_grad\n",
    "    return Y, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0cc23-446d-4a94-9978-151828b8ef0f",
   "metadata": {},
   "source": [
    "## Derivative-enabled Gaussian Process (dGP)\n",
    "\n",
    "We create a custom BoTorch dGP model using GPyTorch. This model requires modifying the mean and covariance matrix to incorporate gradient/derivative information. As such, a mean *vector* and the covariance matrix of the RBF kernel are used [2]. This class was modified from this [GPyTorch tutorial](https://docs.gpytorch.ai/en/latest/examples/08_Advanced_Usage/Simple_GP_Regression_Derivative_Information_1d.html). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42af425-d89b-4398-9bae-fdee07a9121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dGP\n",
    "class GPWithDerivatives(GPyTorchModel, gpytorch.models.ExactGP, FantasizeMixin):\n",
    "    def __init__(self, train_X, train_Y):\n",
    "        # Dimension of model\n",
    "        dim = train_X.shape[-1] \n",
    "        # Multi-dimensional likelihood since we're modeling a function and its gradient\n",
    "        likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=1 + dim)\n",
    "        super().__init__(train_X, train_Y, likelihood)\n",
    "        # Gradient-enabled mean\n",
    "        self.mean_module = gpytorch.means.ConstantMeanGrad() \n",
    "        # Gradient-enabled kernel\n",
    "        self.base_kernel = gpytorch.kernels.RBFKernelGrad( \n",
    "            ard_num_dims=dim, # Separate lengthscale for each input dimension\n",
    "        )\n",
    "        # Adds lengthscale to the kernel\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(self.base_kernel)\n",
    "        # Output dimension is 1 (function value) + dim (number of partial derivatives)\n",
    "        self._num_outputs = 1 + dim\n",
    "        # Used to extract function value and not gradients during optimization\n",
    "        self.scale_tensor = torch.tensor([1.0] + [0.0]*dim, dtype=torch.double)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8024cb-34da-4c16-870e-5dcd9d1b7197",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "\n",
    "To initialize our GP we obtain initialization data using random search on the optimization domain $[-2,2]^3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18331985-8df4-49fb-9ccd-aa681a52995a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search locations\n",
    "train_X = torch.rand((rs_init, rb_dim))*(d_max - d_min)+d_min\n",
    "\n",
    "# Populate random search evaluations\n",
    "train_Y = torch.empty((rs_init, rb_dim + 1))\n",
    "for i in range(rs_init):\n",
    "    obj, deriv = Rosenbrock(train_X[i])\n",
    "    train_Y[i][0] = obj\n",
    "    train_Y[i][1:] = deriv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805acd60-bcd7-425d-ba57-7969d893f453",
   "metadata": {},
   "source": [
    "## qNEI FOBO Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3371766-c05f-4a8c-9720-9f236203a925",
   "metadata": {},
   "source": [
    "Here we have the FOBO loop with the qNEI acquisition function on a dGP. Note that we have to include a `ScalarizedPosteriorTransform` to ensure that we're optiming over the black-box function's function evaluation instead of its gradients (since the output of the dGP is a mean vector of the black-box function's value and gradients, as opposed to a mean scalar of just the black-box function's value). Also, we scale the domain and range of the training data [to avoid numerical issues](https://github.com/pytorch/botorch/issues/1745)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d2138-81ec-49da-8fca-06991197a95c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(bo_iters):\n",
    "    # Standardize domain and range, this prevents numerical issues\n",
    "    mean_Y = train_Y.mean(dim=0)\n",
    "    std_Y = train_Y.std(dim=0)\n",
    "    unscaled_train_Y = train_Y\n",
    "    scaled_train_Y = (train_Y - mean_Y) / std_Y\n",
    "    \n",
    "    mean_X = train_X.mean(dim=0)\n",
    "    std_X = train_X.std(dim=0)\n",
    "    unscaled_train_X = train_X\n",
    "    scaled_train_X = (train_X - mean_X) / std_X\n",
    "\n",
    "    # Initialize the dGP and fit it to the training data\n",
    "    dGP_model = GPWithDerivatives(scaled_train_X, scaled_train_Y) # Define dGP model\n",
    "    mll = ExactMarginalLogLikelihood(dGP_model.likelihood, dGP_model) # Define MLL\n",
    "    fit_gpytorch_mll(mll, max_attempts=20)\n",
    "\n",
    "    # Extract only the function value from the multi-output GP, the dGP\n",
    "    scal_transf = ScalarizedPosteriorTransform(weights=dGP_model.scale_tensor)\n",
    "\n",
    "    # Create qNEI acquisition function\n",
    "    sampler = SobolQMCNormalSampler(mc_samples)\n",
    "    qNEI = qNoisyExpectedImprovement(dGP_model,\\\n",
    "                train_X,\\\n",
    "                sampler,\\\n",
    "                posterior_transform=scal_transf)\n",
    "\n",
    "    # Set bounds for optimization: [-2,2]^d\n",
    "    bounds = torch.vstack([torch.tensor([d_min]*rb_dim),\\\n",
    "                           torch.tensor([d_max]*rb_dim)])\n",
    "\n",
    "    # Rescale bounds based on training data\n",
    "    bounds = (bounds - mean_X) / std_X\n",
    "\n",
    "    # Get candidate point for objective\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=qNEI,\n",
    "        bounds=bounds,\n",
    "        q=1,\n",
    "        num_restarts=100,\n",
    "        raw_samples=512,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 1, \"maxiter\": 1000},\n",
    "    )\n",
    "\n",
    "    # Rescale candidate back to original domain\n",
    "    candidate = (candidates[0]  * std_X) + mean_X\n",
    "    \n",
    "    # Evaluate the objective and add it to the list of data for the model\n",
    "    obj, deriv = Rosenbrock(candidate)\n",
    "    new_Y = torch.cat([obj.unsqueeze(0),deriv])\n",
    "    \n",
    "    # Append evaluation to training data\n",
    "    train_X = torch.vstack((train_X, candidate)).detach().clone()\n",
    "    train_Y = torch.vstack((train_Y, new_Y)).detach().clone()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a9838-1b0d-4369-b0cf-0baffe996925",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d9ca3-2fa1-4df0-b1ea-b33b3e46f24e",
   "metadata": {},
   "source": [
    "From plotting the results we see that derivative-enabled Bayesian Optimization is able to utilize gradient information to maximize the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b235183-05ed-428c-b32d-2a8da25bb296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract maximum value per iteration\n",
    "maxima = torch.zeros(rs_init + bo_iters)\n",
    "cur_max = train_Y[0][0]\n",
    "\n",
    "for i in range(rs_init + bo_iters):\n",
    "    cur_max = cur_max if cur_max > train_Y[i][0] else train_Y[i][0]\n",
    "    maxima[i] = cur_max\n",
    "\n",
    "# Get plotting values\n",
    "plt_y = maxima.numpy()\n",
    "plt_x = list(range(1,len(plt_y)+1))\n",
    "\n",
    "# Have the first x-value in the plot start at 1 to be consistent with above prints\n",
    "plt_y = np.hstack([plt_y])\n",
    "plt_x = np.hstack([plt_x])\n",
    "\n",
    "# Plot all values\n",
    "plt.plot([plt_x[rs_init-1], plt_x[rs_init]], [plt_y[rs_init-1], plt_y[rs_init]],\\\n",
    "         linestyle='--', color='gray')\n",
    "plt.plot(plt_x[0:rs_init], plt_y[0:rs_init], color='red', label=\"Init values\", marker='.')\n",
    "plt.plot(plt_x[rs_init:], plt_y[rs_init:], color='black', label=\"BO values\", marker='.')\n",
    "plt.xlabel(\"BO Iteration\")\n",
    "plt.ylabel(\"Maximum value\")\n",
    "plt.title(\"Best value found per BO iteration including initialization\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot([plt_x[rs_init-1], plt_x[rs_init]], [plt_y[rs_init-1], plt_y[rs_init]],\\\n",
    "         linestyle='--', color='gray')\n",
    "plt.plot(plt_x[rs_init-1], plt_y[rs_init-1], color='red', label=\"Init values\", marker='.')\n",
    "plt.plot(plt_x[rs_init:], plt_y[rs_init:], color='black', label=\"BO values\", marker='.')\n",
    "plt.xlabel(\"BO Iteration\")\n",
    "plt.ylabel(\"Maximum value\")\n",
    "plt.title(\"Best value found per BO iteration after initialization\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8e9fc-d42a-4215-800d-55c51decb920",
   "metadata": {},
   "source": [
    "## qKG FOBO Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc5e991-8508-4050-a1c3-d3da3b14b1cb",
   "metadata": {},
   "source": [
    "Here we have the same FOBO loop, but with the qKG acquisition function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b034797-d2ae-4b73-ba59-d1f94fa88e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reset training data to the same as for qNEI\n",
    "train_X = train_X[0:rs_init]\n",
    "train_Y = train_Y[0:rs_init]\n",
    "\n",
    "for i in range(bo_iters):\n",
    "    # Standardize domain and range, this prevents numerical issues\n",
    "    mean_Y = train_Y.mean(dim=0)\n",
    "    std_Y = train_Y.std(dim=0)\n",
    "    unscaled_train_Y = train_Y\n",
    "    scaled_train_Y = (train_Y - mean_Y) / std_Y\n",
    "    \n",
    "    mean_X = train_X.mean(dim=0)\n",
    "    std_X = train_X.std(dim=0)\n",
    "    unscaled_train_X = train_X\n",
    "    scaled_train_X = (train_X - mean_X) / std_X\n",
    "\n",
    "    # Initialize the dGP and fit it to the training data\n",
    "    dGP_model = GPWithDerivatives(scaled_train_X, scaled_train_Y) # Define dGP model\n",
    "    mll = ExactMarginalLogLikelihood(dGP_model.likelihood, dGP_model) # Define MLL\n",
    "    fit_gpytorch_mll(mll, max_attempts=20)\n",
    "\n",
    "    # Extract only the function value from the multi-output GP, the dGP\n",
    "    scal_transf = ScalarizedPosteriorTransform(weights=dGP_model.scale_tensor)\n",
    "\n",
    "    # Create the qKG acquisition function\n",
    "    qKG = qKnowledgeGradient(dGP_model,\\\n",
    "                posterior_transform=scal_transf,\\\n",
    "                num_fantasies=5)\n",
    "\n",
    "    # Set bounds for optimization: [-2,2]^d\n",
    "    bounds = torch.vstack([torch.tensor([d_min]*rb_dim),\\\n",
    "                           torch.tensor([d_max]*rb_dim)])\n",
    "\n",
    "    # Rescale domain based on training data\n",
    "    bounds = (bounds - mean_X) / std_X\n",
    "\n",
    "    # Get candidate point for objective\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=qKG,\n",
    "        bounds=bounds,\n",
    "        q=1,\n",
    "        num_restarts=100,\n",
    "        raw_samples=512,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 1, \"maxiter\": 1000},\n",
    "    )\n",
    "\n",
    "    # Rescale candidate back to original domain\n",
    "    candidate = (candidates[0]  * std_X) + mean_X\n",
    "    \n",
    "    # Evaluate the objective and add it to the list of data for the model\n",
    "    obj, deriv = Rosenbrock(candidate)\n",
    "    new_Y = torch.cat([obj.unsqueeze(0),deriv])\n",
    "    \n",
    "    # Append evaluation to training data\n",
    "    train_X = torch.vstack((train_X, candidate)).detach().clone()\n",
    "    train_Y = torch.vstack((train_Y, new_Y)).detach().clone()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d05516-6e89-43a1-8904-5acd950e9f00",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e0aec-424b-4ac4-b908-256eb6eee790",
   "metadata": {},
   "source": [
    "From plotting the results we see that derivative-enabled Bayesian Optimization is able to find better maxima values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f082f-92ac-4c9c-bcdc-8d428f0b6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract maximum value per iteration\n",
    "maxima = torch.zeros(rs_init + bo_iters)\n",
    "cur_max = train_Y[0][0]\n",
    "\n",
    "for i in range(rs_init + bo_iters):\n",
    "    cur_max = cur_max if cur_max > train_Y[i][0] else train_Y[i][0]\n",
    "    maxima[i] = cur_max\n",
    "\n",
    "# Get plotting values\n",
    "plt_y = maxima.numpy()\n",
    "plt_x = list(range(1,len(plt_y)+1))\n",
    "\n",
    "# Have the first x-value in the plot start at 1 to be consistent with above prints\n",
    "plt_y = np.hstack([plt_y])\n",
    "plt_x = np.hstack([plt_x])\n",
    "\n",
    "# Plot all values\n",
    "plt.plot([plt_x[rs_init-1], plt_x[rs_init]], [plt_y[rs_init-1], plt_y[rs_init]],\\\n",
    "         linestyle='--', color='gray')\n",
    "plt.plot(plt_x[0:rs_init], plt_y[0:rs_init], color='red', label=\"Init values\", marker='.')\n",
    "plt.plot(plt_x[rs_init:], plt_y[rs_init:], color='black', label=\"BO values\", marker='.')\n",
    "plt.xlabel(\"BO Iteration\")\n",
    "plt.ylabel(\"Maximum value\")\n",
    "plt.title(\"Best value found per BO iteration including initialization\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.plot([plt_x[rs_init-1], plt_x[rs_init]], [plt_y[rs_init-1], plt_y[rs_init]],\\\n",
    "         linestyle='--', color='gray')\n",
    "plt.plot(plt_x[rs_init-1], plt_y[rs_init-1], color='red', label=\"Init values\", marker='.')\n",
    "plt.plot(plt_x[rs_init:], plt_y[rs_init:], color='black', label=\"BO values\", marker='.')\n",
    "plt.xlabel(\"BO Iteration\")\n",
    "plt.ylabel(\"Maximum value\")\n",
    "plt.title(\"Best value found per BO iteration after initialization\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
